{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import string\n",
    "import pickle\n",
    "import json\n",
    "import traceback\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from google.cloud import storage\n",
    "from google.cloud import vision\n",
    "from google.protobuf import json_format\n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = str(text)\n",
    "    #remove numbers and lowercase\n",
    "    text = re.sub(r'\\d+', '', text.lower())\n",
    "    #remove punctuation\n",
    "    text = ''.join(c for c in text if c not in string.punctuation)\n",
    "    #remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def async_detect_document(gcs_source_uri, gcs_destination_uri):\n",
    "    \"\"\"OCR with PDF/TIFF as source files on GCS\"\"\"\n",
    "    # Supported mime_types are: 'application/pdf' and 'image/tiff'\n",
    "    mime_type = 'application/pdf'\n",
    "\n",
    "    # How many pages should be grouped into each json output file.\n",
    "    batch_size = 2\n",
    "\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    feature = vision.types.Feature(\n",
    "        type=vision.enums.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "\n",
    "    gcs_source = vision.types.GcsSource(uri=gcs_source_uri)\n",
    "    input_config = vision.types.InputConfig(\n",
    "        gcs_source=gcs_source, mime_type=mime_type)\n",
    "\n",
    "    gcs_destination = vision.types.GcsDestination(uri=gcs_destination_uri)\n",
    "    output_config = vision.types.OutputConfig(\n",
    "        gcs_destination=gcs_destination, batch_size=batch_size)\n",
    "\n",
    "    async_request = vision.types.AsyncAnnotateFileRequest(\n",
    "        features=[feature], input_config=input_config,\n",
    "        output_config=output_config)\n",
    "\n",
    "    operation = client.async_batch_annotate_files(\n",
    "        requests=[async_request])\n",
    "\n",
    "    print('Waiting for the operation to finish.')\n",
    "    operation.result(timeout=420)\n",
    "\n",
    "    # Once the request has completed and the output has been\n",
    "    # written to GCS, we can list all the output files.\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    match = re.match(r'gs://([^/]+)/(.+)', gcs_destination_uri)\n",
    "    bucket_name = match.group(1)\n",
    "    prefix = match.group(2)\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # List objects with the given prefix.\n",
    "    blob_list = list(bucket.list_blobs(prefix=prefix))\n",
    "    res = []\n",
    "#     print('Output files:')\n",
    "    for blob in blob_list:\n",
    "        print(blob.name)\n",
    "\n",
    "        # Process the first output file from GCS.\n",
    "        # Since we specified batch_size=2, the first response contains\n",
    "        # the first two pages of the input file.\n",
    "        output = blob\n",
    "\n",
    "        json_string = output.download_as_string()\n",
    "        response = json_format.Parse(\n",
    "            json_string, vision.types.AnnotateFileResponse())\n",
    "\n",
    "        # The actual response for the first page of the input file.\n",
    "        for resp in response.responses:\n",
    "            annotation = resp.full_text_annotation\n",
    "\n",
    "        # Here we print the full text from the first page.\n",
    "        # The response contains more information:\n",
    "        # annotation/pages/blocks/paragraphs/words/symbols\n",
    "        # including confidence scores and bounding boxes\n",
    "    #     print(u'Full text:\\n{}'.format(\n",
    "    #         annotation.text))\n",
    "            res.append(annotation.text)\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_text(path):\n",
    "    \n",
    "    bucket, file_name = path.split('/', 1)\n",
    "    bucket_path = \"gs://\"+ bucket + \"/\" + file_name\n",
    "#     print(bucket_path)\n",
    "    data = []\n",
    "    data.append(async_detect_document(bucket_path, 'gs://s80-dochub-ocr-test-bucket-03/jsons/' + file_name.split('.')[0] + '/'))\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def split_pages(path):\n",
    "\n",
    "    try:\n",
    "    \n",
    "        client = storage.Client()\n",
    "\n",
    "        bucket_name = path.split('/', 1)[0]\n",
    "        prefix = path.split('/', 1)[1]\n",
    "\n",
    "        file_name = prefix.split('/')[-1].split('.')[0]\n",
    "#         regex = re.compile('[^0-9a-zA-Z]')\n",
    "#         file_name = regex.sub('', file_name)\n",
    "#         print('file name: ', file_name)\n",
    "\n",
    "        bucket = client.bucket(bucket_name)\n",
    "\n",
    "        blob = bucket.blob(prefix)\n",
    "\n",
    "        if(not os.path.exists('/tmp/' + 'files')):\n",
    "            os.makedirs('/tmp/' + 'files',exist_ok=True)\n",
    "\n",
    "        blob.download_to_filename('/tmp/files/{}.pdf'.format(file_name))\n",
    "\n",
    "        pdf_document = \"/tmp/files/{}.pdf\".format(file_name)\n",
    "        pdf = PdfFileReader(pdf_document)\n",
    "\n",
    "        files_list = []\n",
    "        upload_bucket = client.bucket('s80-dochub-ocr-test-bucket-03')\n",
    "\n",
    "        for page in range(pdf.getNumPages()):\n",
    "            pdf_writer = PdfFileWriter()\n",
    "            current_page = pdf.getPage(page)\n",
    "            pdf_writer.addPage(current_page)\n",
    "\n",
    "            outputFilename = \"/tmp/files/{}_{}.pdf\".format(file_name, page + 1)\n",
    "            files_list.append('s80-dochub-ocr-test-bucket-03/' + \"files/{}_{}.pdf\".format(file_name, page + 1))\n",
    "            with open(outputFilename, \"wb\") as out:\n",
    "                pdf_writer.write(out)\n",
    "\n",
    "            upload_blob = upload_bucket.blob(\"files/{}_{}.pdf\".format(file_name, page + 1))\n",
    "            upload_blob.upload_from_filename(outputFilename)\n",
    "\n",
    "#             print(\"created\", outputFilename)\n",
    "#             print(files_list)\n",
    "        \n",
    "        files = glob.glob('/tmp/files/*')\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "        return files_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Exception :', e)\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(path):\n",
    "    \"\"\"Triggered by a change to a Cloud Storage bucket.\n",
    "    Args:\n",
    "         event (dict): Event payload.\n",
    "         context (google.cloud.functions.Context): Metadata for the event.\n",
    "    \"\"\"\n",
    "    try:\n",
    "#         file = event\n",
    "#         print(f\"Processing file: {file['name']}.\")\n",
    "\n",
    "#         bucket = file['bucket']\n",
    "#         input_pdf = file['name']\n",
    "\n",
    "#         path = uri.split('//', 1)[1]\n",
    "\n",
    "        files_to_be_processed = split_pages(path)\n",
    "        text_list = []\n",
    "\n",
    "        for file_path in files_to_be_processed:\n",
    "            text_list.extend(get_text(file_path))\n",
    "        \n",
    "        return text_list\n",
    "#             df = pd.DataFrame(df, columns=['text'])\n",
    "#             df['text'] = df['text'].apply(clean_text)\n",
    "#             get_inference(df, file_path)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Exception :', e)\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.bucket('s80-dochub-ocr-bucket-2')\n",
    "folder_name='bank_statements'\n",
    "blobs = bucket.list_blobs(prefix=folder_name+'/')\n",
    "blobs_list = list(blobs)\n",
    "len(blobs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tqdm(blobs_list[1:])\n",
    "processed_list = []\n",
    "if __name__ == \"__main__\":\n",
    "    processed_list.extend(Parallel(n_jobs=num_cores)(delayed(main)('s80-dochub-ocr-bucket-2/'+i.name) for i in inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
